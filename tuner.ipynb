{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 09:26:03,656\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-09-19 09:26:10,942\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import ray, random, os \n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "import pandas as pd\n",
    "ray.init(_temp_dir='/Volumes/SSD980/ray')\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from env.multi_agent.hrl import HRL\n",
    "from ray.train.rl import RLTrainer\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune import TuneConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.air.config import RunConfig, CheckpointConfig\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from ray.rllib.algorithms.a2c import A2C\n",
    "# import optuna\n",
    "# from ray.rllib.algorithms.a2c import A2CConfig\n",
    "\n",
    "# env = HRL()\n",
    "\n",
    "# def env_creator(env_config):\n",
    "#     return HRL(env_config)  \n",
    "\n",
    "# register_env(\"hrl\", env_creator)\n",
    "\n",
    "\n",
    "# def create_policy_spec(worker_id):\n",
    "#     # print(f\"Creating policy for {worker_id} with obs space {env.observation_space[worker_id]} and action space {env.action_space[worker_id]}\")\n",
    "#     return PolicySpec(\n",
    "#         observation_space=env.observation_space[worker_id],\n",
    "#         action_space=env.action_space[worker_id],\n",
    "#         config={}\n",
    "#     )\n",
    "\n",
    " \n",
    "# manager_policy_spec = PolicySpec(\n",
    "#     observation_space=env.observation_space['manager'],\n",
    "#     action_space=env.action_space['manager'],\n",
    "#     config={}\n",
    "# )\n",
    "\n",
    "# policies = {\n",
    "#     \"manager_policy\": manager_policy_spec,\n",
    "# }\n",
    "\n",
    "# for worker_id in env.workers:\n",
    "#     policies[worker_id] = create_policy_spec(worker_id)\n",
    "\n",
    "# def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "#     if agent_id == 'manager':\n",
    "#         # print(f\"!!!!!! policy mapping manager: {agent_id}\")\n",
    "#         return \"manager_policy\"\n",
    "#     elif agent_id in env.workers:\n",
    "#         return agent_id\n",
    "#     else:\n",
    "#         print(\"defaul policy triggered\")\n",
    "#         return \"default_policy\"\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Get the hyperparameters suggested by Optuna\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-4)\n",
    "#     gamma = trial.suggest_float(\"gamma\", 0.95, 0.9999)\n",
    "#     lambda_ = trial.suggest_float(\"lambda\", 0.9, 1.0)\n",
    "#     entropy_coeff = trial.suggest_float(\"entropy_coeff\", 0.01, 0.1)\n",
    "#     vf_loss_coeff = trial.suggest_float(\"vf_loss_coeff\", 0.1, 0.3)\n",
    "\n",
    "#     # Set up the RLlib configuration with the suggested hyperparameters\n",
    "#     config = {\n",
    "#         \"env\": \"hrl\",\n",
    "#         \"multiagent\": {\n",
    "#                 \"policies\": policies,\n",
    "#                 \"policy_mapping_fn\": policy_mapping_fn,\n",
    "#             },\n",
    "#             \"rollout_fragment_length\": \"auto\",\n",
    "#             \"lr\": lr,\n",
    "#             \"gamma\": gamma,\n",
    "#             \"lambda\": lambda_,\n",
    "#             \"entropy_coeff\": entropy_coeff,\n",
    "#             \"vf_loss_coeff\": vf_loss_coeff,\n",
    "#             \"num_workers\": 4,  \n",
    "#             \"log_level\": \"ERROR\",\n",
    "#             \"output\": \"logdir\",\n",
    "#             \"monitor\": True,\n",
    "#     }\n",
    "\n",
    "#     # Create and train an A2C agent directly using RLlib's Python API\n",
    "#     # agent = a2c.A2CTrainer(config=config)\n",
    "#     # results = agent.train()\n",
    "#     config = A2CConfig()\n",
    "\n",
    "#     # Configuring the A2CConfig instance\n",
    "#     config = config.training(lr=lr, gamma=gamma)  \n",
    "#     config = config.resources(num_gpus=0)\n",
    "#     config = config.rollouts(num_rollout_workers=4, rollout_fragment_length=\"auto\")\n",
    "#     config = config.environment(env=\"hrl\")  # assuming you registered your env with the name 'hrl'\n",
    "\n",
    "#     config_dict = config.to_dict()\n",
    "    \n",
    "#     # Ensure the learning rate is correctly set as a float\n",
    "#     config_dict['training']['lr'] = lr\n",
    "\n",
    "#     config_dict = config.to_dict()\n",
    "#     config_dict[\"multiagent\"] = {\n",
    "#         \"policies\": policies,\n",
    "#         \"policy_mapping_fn\": policy_mapping_fn,\n",
    "#     }\n",
    "    \n",
    "#     # Building the algorithm instance from the configured config dictionary\n",
    "#     algo = A2C(config=config_dict)\n",
    "\n",
    "#     # Running a training iteration\n",
    "#     results = algo.train() # 2. build the algorithm,\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"type of results\",type(results))  # This will print the type of the results object\n",
    "#     print(\"results\",results) \n",
    "    \n",
    "#     # Get the mean episode reward from the results\n",
    "#     # Note: you might need to adjust this line to get the correct metric\n",
    "#     mean_episode_reward = results['episode_reward_mean']\n",
    "\n",
    "#     return mean_episode_reward\n",
    "\n",
    "# # Create a study object and specify the direction is 'maximize'.\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# # Optimize the study, the objective function is passed in as the first argument.\n",
    "# study.optimize(objective, n_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.observation_space` for environment variables or `env.get_wrapper_attr('observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dict' object has no attribute 'observation_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Redirect the standard output to the log file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m sys\u001b[39m.\u001b[39mstdout \u001b[39m=\u001b[39m log_file\n\u001b[0;32m---> 13\u001b[0m env \u001b[39m=\u001b[39m HRL()\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menv_creator\u001b[39m(env_config):\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m HRL(env_config)  \n",
      "File \u001b[0;32m~/Documents/GitHub.nosync/quantumai3/env/multi_agent/hrl.py:44\u001b[0m, in \u001b[0;36mHRL.__init__\u001b[0;34m(self, env_config, print_verbosity, initial_capital)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_annualized_return \u001b[39m=\u001b[39m \u001b[39m0.15\u001b[39m  \u001b[39m# Example target annualized return of 15%\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_sharpe_ratio \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m   \n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mDict({\n\u001b[0;32m---> 44\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mmanager\u001b[39m\u001b[39m\"\u001b[39m: wrappers\u001b[39m.\u001b[39;49mNormalizeObservation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmanager\u001b[39m.\u001b[39;49mobservation_space)},\n\u001b[1;32m     45\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{tic: wrappers\u001b[39m.\u001b[39mNormalizeObservation(worker\u001b[39m.\u001b[39mobservation_space) \u001b[39mfor\u001b[39;00m tic, worker \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     46\u001b[0m })\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mDict({\n\u001b[1;32m     51\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mmanager\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanager\u001b[39m.\u001b[39maction_space},\n\u001b[1;32m     52\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{tic: worker\u001b[39m.\u001b[39maction_space \u001b[39mfor\u001b[39;00m tic, worker \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     53\u001b[0m })\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/wrappers/normalize.py:76\u001b[0m, in \u001b[0;36mNormalizeObservation.__init__\u001b[0;34m(self, env, epsilon)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_rms \u001b[39m=\u001b[39m RunningMeanStd(shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingle_observation_space\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     75\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_rms \u001b[39m=\u001b[39m RunningMeanStd(shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39m=\u001b[39m epsilon\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dict' object has no attribute 'observation_space'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:52,292\tWARNING worker.py:2037 -- The log monitor on node Florians-MacBook-Pro.local failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/log_monitor.py\", line 369, in check_log_files_and_publish_updates\n",
      "    next_line = file_info.file_handle.readline()\n",
      "OSError: [Errno 5] Input/output error\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/log_monitor.py\", line 564, in <module>\n",
      "    log_monitor.run()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/log_monitor.py\", line 474, in run\n",
      "    anything_published = self.check_log_files_and_publish_updates()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/log_monitor.py\", line 419, in check_log_files_and_publish_updates\n",
      "    f\"position: {file_info.file_info.file_handle.tell()} \"\n",
      "AttributeError: 'LogFileInfo' object has no attribute 'file_info'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mit vielen policies\n",
    "import sys\n",
    "# Redirect the standard error to the log file\n",
    "\n",
    "# Create a log file\n",
    "log_file = open(\"console_logs.txt\", \"w\")\n",
    "\n",
    "\n",
    "# Redirect the standard output to the log file\n",
    "sys.stdout = log_file\n",
    "\n",
    "\n",
    "env = HRL()\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def create_policy_spec(worker_id):\n",
    "    # print(f\"Creating policy for {worker_id} with obs space {env.observation_space[worker_id]} and action space {env.action_space[worker_id]}\")\n",
    "    return PolicySpec(\n",
    "        observation_space=env.observation_space[worker_id],\n",
    "        action_space=env.action_space[worker_id],\n",
    "        config={}\n",
    "    )\n",
    "\n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "for worker_id in env.workers:\n",
    "    policies[worker_id] = create_policy_spec(worker_id)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id == 'manager':\n",
    "        # print(f\"!!!!!! policy mapping manager: {agent_id}\")\n",
    "        return \"manager_policy\"\n",
    "    elif agent_id in env.workers:\n",
    "        return agent_id\n",
    "    else:\n",
    "        print(\"defaul policy triggered\")\n",
    "        return \"default_policy\"\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "    \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"gamma\": tune.uniform(0.95, 0.9999),\n",
    "        \"lambda\": tune.uniform(0.9,1.0),\n",
    "        \"entropy_coeff\": tune.uniform(0.01,0.1),\n",
    "        \"vf_loss_coeff\": tune.uniform(0.1,0.3),\n",
    "        \"num_workers\": 3, \n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"output\": \"logdir\",\n",
    "        \"monitor\": True,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"A2C\", \n",
    "    metric=\"episode_reward_mean\", \n",
    "    num_samples=10,\n",
    "    mode=\"max\",\n",
    "    config=param_space, \n",
    "    storage_path=\"/Volumes/SSD980/ray/results/test_tunerun4\",\n",
    "    search_alg=None,\n",
    "    scheduler=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10,max_report_frequency=120),\n",
    "    max_concurrent_trials=1,\n",
    "    #checkpoint_config not checked yet\n",
    "    checkpoint_config={\n",
    "        \"num_to_keep\": 3,\n",
    "        \"checkpoint_score_attribute\": \"episode_reward_mean\",\n",
    "        \"checkpoint_score_order\": \"max\",\n",
    "        \"checkpoint_frequency\": 10\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-17 23:49:34,054\tWARNING hyperopt_search.py:194 -- You passed a `space` parameter to <class 'ray.tune.search.hyperopt.hyperopt_search.HyperOptSearch'> that contained unresolved search space definitions. <class 'ray.tune.search.hyperopt.hyperopt_search.HyperOptSearch'> should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `param_space` argument to `tune.Tuner()` instead.\n",
      "2023-09-17 23:49:34,273\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2023-09-17 23:49:34,300\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-09-17 23:49:34,303\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "2023-09-17 23:49:34,329\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-09-17 23:49:34,374\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-09-17 23:49:39 (running for 00:00:04.61)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /Users/floriankockler/Documents/GitHub.nosync/raystorage/A2C\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "+---------------------+----------+-------+-----------------+----------+-------------+-----------------+\n",
      "| Trial name          | status   | loc   |   entropy_coeff |   lambda |          lr |   vf_loss_coeff |\n",
      "|---------------------+----------+-------+-----------------+----------+-------------+-----------------|\n",
      "| A2C_hrl_16ff4_00000 | PENDING  |       |      0.0924146  | 0.99105  | 4.36633e-05 |        0.173014 |\n",
      "| A2C_hrl_16ff4_00001 | PENDING  |       |      0.0457276  | 0.986487 | 9.79756e-05 |        0.251909 |\n",
      "| A2C_hrl_16ff4_00002 | PENDING  |       |      0.0448883  | 0.968225 | 6.63677e-05 |        0.422413 |\n",
      "| A2C_hrl_16ff4_00003 | PENDING  |       |      0.0286173  | 0.988839 | 6.28478e-05 |        0.413676 |\n",
      "| A2C_hrl_16ff4_00004 | PENDING  |       |      0.0919845  | 0.977094 | 1.99905e-05 |        0.902977 |\n",
      "| A2C_hrl_16ff4_00005 | PENDING  |       |      0.018722   | 0.967426 | 2.58108e-05 |        0.517458 |\n",
      "| A2C_hrl_16ff4_00006 | PENDING  |       |      0.0499366  | 0.995835 | 9.77822e-05 |        0.26068  |\n",
      "| A2C_hrl_16ff4_00007 | PENDING  |       |      0.0120254  | 0.96368  | 5.79893e-05 |        0.980071 |\n",
      "| A2C_hrl_16ff4_00008 | PENDING  |       |      0.00666563 | 0.970509 | 8.92096e-05 |        0.831585 |\n",
      "| A2C_hrl_16ff4_00009 | PENDING  |       |      0.0468594  | 0.953478 | 2.48187e-05 |        0.237312 |\n",
      "+---------------------+----------+-------+-----------------+----------+-------------+-----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17991)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m 2023-09-17 23:49:49,017\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m 2023-09-17 23:49:49,017\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m 2023-09-17 23:49:49,018\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=18012)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:49:49,027\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:49:49,028\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,824\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,824\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,855\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,856\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,856\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,857\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,857\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,857\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,876\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchMultiActionDistribution` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchMultiDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,877\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m 2023-09-17 23:49:59,878\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDiagGaussian` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchDiagGaussian` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=18012)\u001b[0m action_dict HRL:  {'manager': OrderedDict([('ABT.US', OrderedDict([('capital_allocation', array([0.8462131], dtype=float32))])), ('AMGN.US', OrderedDict([('capital_allocation', array([0.28759483], dtype=float32))])), ('BDX.US', OrderedDict([('capital_allocation', array([0.6832139], dtype=float32))])), ('BMY.US', OrderedDict([('capital_allocation', array([0.9392452], dtype=float32))])), ('HUM.US', OrderedDict([('capital_allocation', array([0.56413096], dtype=float32))])), ('JNJ.US', OrderedDict([('capital_allocation', array([0.81873757], dtype=float32))])), ('LLY.US', OrderedDict([('capital_allocation', array([0.33888644], dtype=float32))])), ('MDT.US', OrderedDict([('capital_allocation', array([0.10934579], dtype=float32))])), ('MRK.US', OrderedDict([('capital_allocation', array([0.0227138], dtype=float32))])), ('PFE.US', OrderedDict([('capital_allocation', array([0.45026165], dtype=float32))])), ('SYK.US', OrderedDict([('capital_allocation', array([0.6096404], dtype=float32))])), ('TMO.US', OrderedDict([('capital_allocation', array([0.46824914], dtype=float32))])), ('UNH.US', OrderedDict([('capital_allocation', array([0.18529464], dtype=float32))]))]), 'ABT.US': OrderedDict([('amount', array([0.4271011], dtype=float32)), ('type', 0)]), 'AMGN.US': OrderedDict([('amount', array([0.86916846], dtype=float32)), ('type', 0)]), 'BDX.US': OrderedDict([('amount', array([0.1153217], dtype=float32)), ('type', 0)]), 'BMY.US': OrderedDict([('amount', array([0.93552655], dtype=float32)), ('type', 1)]), 'HUM.US': OrderedDict([('amount', array([0.77198994], dtype=float32)), ('type', 1)]), 'JNJ.US': OrderedDict([('amount', array([0.9982008], dtype=float32)), ('type', 1)]), 'LLY.US': OrderedDict([('amount', array([0.74063057], dtype=float32)), ('type', 1)]), 'MDT.US': OrderedDict([('amount', array([0.731855], dtype=float32)), ('type', 2)]), 'MRK.US': OrderedDict([('amount', array([0.06597856], dtype=float32)), ('type', 1)]), 'PFE.US': OrderedDict([('amount', array([0.91958296], dtype=float32)), ('type', 0)]), 'SYK.US': OrderedDict([('amount', array([0.1063927], dtype=float32)), ('type', 2)]), 'TMO.US': OrderedDict([('amount', array([0.53522927], dtype=float32)), ('type', 1)]), 'UNH.US': OrderedDict([('amount', array([0.7690391], dtype=float32)), ('type', 2)])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m Trainable.setup took 11.282 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-09-17 23:50:00,780\tERROR tune_controller.py:911 -- Trial task failed for trial A2C_hrl_16ff4_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/worker.py\", line 2524, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::A2C.train()\u001b[39m (pid=17991, ip=127.0.0.1, actor_id=16bf198d6f9d3a1beb02c67101000000, repr=A2C)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 375, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 372, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 853, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2837, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/a2c/a2c.py\", line 181, in training_step\n",
      "    return Algorithm.training_step(self)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 1425, in training_step\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 85, in synchronous_parallel_sample\n",
      "    sample_batches = worker_set.foreach_worker(\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 680, in foreach_worker\n",
      "    handle_remote_call_result_errors(remote_results, self._ignore_worker_failures)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 76, in handle_remote_call_result_errors\n",
      "    raise r.get()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=18014, ip=127.0.0.1, actor_id=4cfac9d0be4944b3b0e65f9c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17d0ffd00>)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 696, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n",
      "    outputs = self.step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 400, in step\n",
      "    self._base_env.send_actions(actions_to_send)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n",
      "    raise e\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n",
      "    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n",
      "  File \"/Users/floriankockler/Documents/GitHub.nosync/quantumai3/env/multi_agent/hrl.py\", line 88, in step\n",
      "    manager_action = action_dict[\"manager\"]\n",
      "KeyError: 'manager'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=18014)\u001b[0m action_dict HRL:  {'manager': {'ABT.US': {'capital_allocation': array([1.], dtype=float32)}, 'AMGN.US': {'capital_allocation': array([0.64362574], dtype=float32)}, 'BDX.US': {'capital_allocation': array([0.49731275], dtype=float32)}, 'BMY.US': {'capital_allocation': array([1.], dtype=float32)}, 'HUM.US': {'capital_allocation': array([1.], dtype=float32)}, 'JNJ.US': {'capital_allocation': array([0.55362946], dtype=float32)}, 'LLY.US': {'capital_allocation': array([0.], dtype=float32)}, 'MDT.US': {'capital_allocation': array([0.], dtype=float32)}, 'MRK.US': {'capital_allocation': array([0.23150986], dtype=float32)}, 'PFE.US': {'capital_allocation': array([0.24992993], dtype=float32)}, 'SYK.US': {'capital_allocation': array([0.09568444], dtype=float32)}, 'TMO.US': {'capital_allocation': array([0.], dtype=float32)}, 'UNH.US': {'capital_allocation': array([0.54873806], dtype=float32)}}, 'ABT.US': {'amount': array([0.7603689], dtype=float32), 'type': 2}, 'AMGN.US': {'amount': array([0.5975817], dtype=float32), 'type': 1}, 'BDX.US': {'amount': array([0.344153], dtype=float32), 'type': 1}, 'BMY.US': {'amount': array([0.], dtype=float32), 'type': 0}, 'HUM.US': {'amount': array([0.8442199], dtype=float32), 'type': 0}, 'JNJ.US': {'amount': array([0.7624078], dtype=float32), 'type': 2}, 'LLY.US': {'amount': array([0.45497847], dtype=float32), 'type': 1}, 'MDT.US': {'amount': array([0.833815], dtype=float32), 'type': 2}, 'MRK.US': {'amount': array([0.93175554], dtype=float32), 'type': 2}, 'PFE.US': {'amount': array([0.30208755], dtype=float32), 'type': 2}, 'SYK.US': {'amount': array([1.], dtype=float32), 'type': 2}, 'TMO.US': {'amount': array([0.79685724], dtype=float32), 'type': 2}, 'UNH.US': {'amount': array([0.86414653], dtype=float32), 'type': 2}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18014)\u001b[0m action_dict HRL:  {'ABT.US': {'amount': array([0.1670942], dtype=float32), 'type': 1}, 'AMGN.US': {'amount': array([0.], dtype=float32), 'type': 0}, 'BDX.US': {'amount': array([1.], dtype=float32), 'type': 0}, 'BMY.US': {'amount': array([0.45633698], dtype=float32), 'type': 1}, 'HUM.US': {'amount': array([0.], dtype=float32), 'type': 2}, 'JNJ.US': {'amount': array([0.5860411], dtype=float32), 'type': 1}, 'LLY.US': {'amount': array([0.98430693], dtype=float32), 'type': 2}, 'MDT.US': {'amount': array([0.71871215], dtype=float32), 'type': 1}, 'MRK.US': {'amount': array([0.4732288], dtype=float32), 'type': 2}, 'PFE.US': {'amount': array([0.59913206], dtype=float32), 'type': 0}, 'SYK.US': {'amount': array([0.60776955], dtype=float32), 'type': 0}, 'TMO.US': {'amount': array([1.], dtype=float32), 'type': 1}, 'UNH.US': {'amount': array([0.15976906], dtype=float32), 'type': 2}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m 2023-09-17 23:50:00,765\tERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=18014, ip=127.0.0.1, actor_id=4cfac9d0be4944b3b0e65f9c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17d0ffd00>)\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     outputs = self.step()\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 400, in step\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     self._base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m   File \"/Users/floriankockler/Documents/GitHub.nosync/quantumai3/env/multi_agent/hrl.py\", line 88, in step\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m     manager_action = action_dict[\"manager\"]\n",
      "\u001b[2m\u001b[36m(A2C pid=17991)\u001b[0m KeyError: 'manager'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_hrl_16ff4_00000</td></tr>\n",
       "<tr><td>A2C_hrl_16ff4_00001</td></tr>\n",
       "<tr><td>A2C_hrl_16ff4_00002</td></tr>\n",
       "<tr><td>A2C_hrl_16ff4_00003</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-17 23:50:00,965\tERROR tune_controller.py:911 -- Trial task failed for trial A2C_hrl_16ff4_00003\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/worker.py\", line 2524, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::A2C.train()\u001b[39m (pid=17992, ip=127.0.0.1, actor_id=7119389739db29cc889af6ef01000000, repr=A2C)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 375, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 372, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 853, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2837, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/a2c/a2c.py\", line 181, in training_step\n",
      "    return Algorithm.training_step(self)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 1425, in training_step\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 85, in synchronous_parallel_sample\n",
      "    sample_batches = worker_set.foreach_worker(\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 680, in foreach_worker\n",
      "    handle_remote_call_result_errors(remote_results, self._ignore_worker_failures)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 76, in handle_remote_call_result_errors\n",
      "    raise r.get()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=18012, ip=127.0.0.1, actor_id=f0817f0cbe73fc1a2357e5d401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17a553cd0>)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 696, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n",
      "    outputs = self.step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 400, in step\n",
      "    self._base_env.send_actions(actions_to_send)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n",
      "    raise e\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n",
      "    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n",
      "  File \"/Users/floriankockler/Documents/GitHub.nosync/quantumai3/env/multi_agent/hrl.py\", line 88, in step\n",
      "    manager_action = action_dict[\"manager\"]\n",
      "KeyError: 'manager'\n",
      "2023-09-17 23:50:01,041\tERROR tune_controller.py:911 -- Trial task failed for trial A2C_hrl_16ff4_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/worker.py\", line 2524, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::A2C.train()\u001b[39m (pid=17989, ip=127.0.0.1, actor_id=c4c30d7aa59ebfb8462f899701000000, repr=A2C)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 375, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 372, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 853, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2837, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/a2c/a2c.py\", line 181, in training_step\n",
      "    return Algorithm.training_step(self)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 1425, in training_step\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 85, in synchronous_parallel_sample\n",
      "    sample_batches = worker_set.foreach_worker(\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 680, in foreach_worker\n",
      "    handle_remote_call_result_errors(remote_results, self._ignore_worker_failures)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 76, in handle_remote_call_result_errors\n",
      "    raise r.get()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=18015, ip=127.0.0.1, actor_id=b28734f39e44f2bdbb1e0be101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17bc77ca0>)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 696, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n",
      "    outputs = self.step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 400, in step\n",
      "    self._base_env.send_actions(actions_to_send)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n",
      "    raise e\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n",
      "    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n",
      "  File \"/Users/floriankockler/Documents/GitHub.nosync/quantumai3/env/multi_agent/hrl.py\", line 88, in step\n",
      "    manager_action = action_dict[\"manager\"]\n",
      "KeyError: 'manager'\n",
      "2023-09-17 23:50:01,155\tERROR tune_controller.py:911 -- Trial task failed for trial A2C_hrl_16ff4_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/_private/worker.py\", line 2524, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::A2C.train()\u001b[39m (pid=17990, ip=127.0.0.1, actor_id=b4c223e99ab176581be8855501000000, repr=A2C)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 375, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 372, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 853, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2837, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/a2c/a2c.py\", line 181, in training_step\n",
      "    return Algorithm.training_step(self)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 1425, in training_step\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 85, in synchronous_parallel_sample\n",
      "    sample_batches = worker_set.foreach_worker(\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 680, in foreach_worker\n",
      "    handle_remote_call_result_errors(remote_results, self._ignore_worker_failures)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 76, in handle_remote_call_result_errors\n",
      "    raise r.get()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=18013, ip=127.0.0.1, actor_id=d140bb407c0db4dc4531da9401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17b0bfcd0>)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 696, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n",
      "    outputs = self.step()\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 400, in step\n",
      "    self._base_env.send_actions(actions_to_send)\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n",
      "    raise e\n",
      "  File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n",
      "    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n",
      "  File \"/Users/floriankockler/Documents/GitHub.nosync/quantumai3/env/multi_agent/hrl.py\", line 88, in step\n",
      "    manager_action = action_dict[\"manager\"]\n",
      "KeyError: 'manager'\n",
      "\u001b[2m\u001b[36m(pid=18064)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,228\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,229\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,240\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,240\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,241\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,241\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,241\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,241\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,261\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchMultiActionDistribution` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchMultiDistribution` instead. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,261\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.` instead. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,262\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDiagGaussian` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchDiagGaussian` instead. This will raise an error in the future!\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m Trainable.setup took 11.313 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m 2023-09-17 23:50:00,987\tERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=18013, ip=127.0.0.1, actor_id=d140bb407c0db4dc4531da9401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17b0bfcd0>)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     raise e\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     lambda w: w.sample(), local_worker=False, healthy_only=True\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 696, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     batches = [self.input_reader.next()]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     batches = [self.get_data()]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     item = next(self._env_runner)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     outputs = self.step()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m   File \"/Users/floriankockler/Documents/GitHub.nosync/quantumai3/env/multi_agent/hrl.py\", line 88, in step\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     self._base_env.send_actions(actions_to_send)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m   File \"/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m     manager_action = action_dict[\"manager\"]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=17990)\u001b[0m KeyError: 'manager'\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(A2C pid=18064)\u001b[0m 2023-09-17 23:50:11,245\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=18064)\u001b[0m 2023-09-17 23:50:11,245\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=18064)\u001b[0m 2023-09-17 23:50:11,246\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n"
     ]
    }
   ],
   "source": [
    "env = HRL()\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "initial_params = [{\n",
    "    \"lr\": 0.001,\n",
    "    \"gamma\": 0.92,\n",
    "    \"lambda\": 0.95,\n",
    "    \"entropy_coeff\": 1e-3,\n",
    "    \"vf_loss_coeff\": 0.5,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": 64,\n",
    "        \"fcnet_activation\":\"relu\",\n",
    "    },\n",
    "}]\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),\n",
    "    \"lambda\": tune.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "    # \"model\": {\n",
    "    #     \"fcnet_hiddens\": tune.grid_search([[64, 64], [128, 128], [256, 256]]),\n",
    "    #     \"fcnet_activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    # },\n",
    "}\n",
    "\n",
    "\n",
    "algo = HyperOptSearch(space=search_space,metric=\"episode_reward_mean\", mode=\"max\",)\n",
    "\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "         \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),\n",
    "    \"lambda\": tune.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "          \"lr\": tune.uniform(1e-5,1e-4),\n",
    "          \"lambda\": tune.uniform(0.95, 1.0),\n",
    "          \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "          \"entropy_coeff\": tune.uniform(1e-4, 1e-1),\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_envs_per_worker\": 1\n",
    "}\n",
    "\n",
    "analysis = tune.run(\"A2C\", \n",
    "                    metric=\"episode_reward_mean\", \n",
    "                    mode=\"max\",\n",
    "                    config=param_space,\n",
    "                    num_samples=10,\n",
    "                    stop={\"training_iteration\": 100},\n",
    "                    progress_reporter=CLIReporter(max_progress_rows=10,max_report_frequency=600),\n",
    "                    # local_dir=\"/Users/floriankockler/rayresults/overnight1\",\n",
    "                    storage_path=\"/Users/floriankockler/Documents/GitHub.nosync/raystorage\",\n",
    "                    checkpoint_config=CheckpointConfig(\n",
    "                        num_to_keep=2,\n",
    "                        checkpoint_score_attribute=\"episode_reward_mean\", \n",
    "                        checkpoint_score_order=\"max\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "best = analysis.best_trial\n",
    "print(pretty_print(best.last_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = HRL()\n",
    "\n",
    "n_iterations = 1\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    obs, reward, done, truncated, info= env.step(action)\n",
    "\n",
    "    # print(f\"Action: {action}, Reward: {reward}, Portfolio Value: {obs[0] + obs[1] * obs[2]}\")\n",
    "    \n",
    "    if done[\"__all__\"]:\n",
    "        print(\"Episode finished!\")\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "    \n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn = policy_mapping_fn,  \n",
    "    ).training(train_batch_size=4000).build()\n",
    "\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRL()\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "          \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\"A2C\", metric=\"episode_reward_mean\", mode=\"max\",config=param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "    policies={\n",
    "        \"policy_1\": ()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import CLIReporter\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "env = HRL()\n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune_config,\n",
    "    param_space=param_space,\n",
    "\n",
    "    run_config=run_config,\n",
    ")\n",
    "results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from env.multi_agent.hrl import HRL\n",
    "\n",
    "reporter = CLIReporter(max_progress_rows=10)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "manager_config = {\n",
    "    \"df\": train_df,\n",
    "\n",
    "}\n",
    "hrl_config={\n",
    "        \"manager_config\": manager_config\n",
    "        }\n",
    "env = HRL(hrl_config)\n",
    " \n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    " \n",
    "\n",
    "\n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\"\n",
    ")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "\n",
    "def explore(config):\n",
    "    # Ensure we collect enough timesteps to do sgd\n",
    "    if config[\"train_batch_size\"] < config[\"rollout_fragment_length\"] * 2:\n",
    "        config[\"train_batch_size\"] = config[\"rollout_fragment_length\"] * 2\n",
    "    return config\n",
    "\n",
    "hyperparam_mutations = {\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "    \"gamma\": lambda: random.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": [0.01, 0.1, 1.0],\n",
    "    \"num_envs_per_worker\": [1, 2, 4, 8],\n",
    "    #\"rollout_fragment_length\": [50, 100, 200, 400],\n",
    "    \"train_batch_size\": lambda: random.randint(200, 1500),\n",
    "    \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "\n",
    "}\n",
    "\n",
    "pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        perturbation_interval=120,\n",
    "        resample_probability=0.25,\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations=hyperparam_mutations,\n",
    "        custom_explore_fn=explore,\n",
    "    )\n",
    "\n",
    "# Stop when we've reached 100 training iterations or reward=300\n",
    "stopping_criteria = {\"training_iteration\": 100}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        scheduler=pbt,\n",
    "        num_samples=1 if args.smoke_test else 10,\n",
    "    ),\n",
    "    param_space={\n",
    "        \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"env_config\": hrl_config,\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"num_workers\": 1,  # 1 for training + 4 for sampling\n",
    "        \"num_cpus_per_trial\": 3,\n",
    "        # \"num_cpus\": 1,  # number of CPUs to use per trial --> 6 in total = max available\n",
    "        # \"num_gpus\": 0,  # number of GPUs to use per trial\n",
    "        # These params are tuned from a fixed starting value.\n",
    "        \"lr\": 1e-4,\n",
    "        # These params start off randomly drawn from a set.\n",
    "        \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "        \"train_batch_size\": tune.choice([200, 400, 600]),\n",
    "    },\n",
    "\n",
    "    run_config=air.RunConfig(stop=stopping_criteria, local_dir=\"/Users/floriankockler/rayresults/autobatch\", progress_reporter=reporter),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
